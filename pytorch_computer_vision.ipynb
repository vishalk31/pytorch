{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOD5Q2Lh7G8ReQFfsSTV2Bt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vishalk31/pytorch/blob/main/pytorch_computer_vision.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Load the dataset\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "train_dataset = datasets.FashionMNIST(root='./data', train=True,\n",
        "                             download=True, transform=transform)\n",
        "test_dataset = datasets.FashionMNIST(root='./data', train=False,\n",
        "                             download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64,\n",
        "                          shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64,\n",
        "                          shuffle=False)\n",
        "\n",
        "# Define the model\n",
        "class FashionMNISTModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FashionMNISTModel, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 10),\n",
        "            nn.LogSoftmax(dim=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "model = FashionMNISTModel()\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "loss_function = nn.NLLLoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# Train the model\n",
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "# Training process\n",
        "epochs = 5\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_loader, model, loss_function, optimizer)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WAi6aHeVd6U",
        "outputId": "b7910988-7ae1-40a7-f4fa-e171c68ecd5f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:01<00:00, 20.9MB/s]\n",
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 339kB/s]\n",
            "100%|██████████| 4.42M/4.42M [00:00<00:00, 6.27MB/s]\n",
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 9.16MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.341174 [    0/60000]\n",
            "loss: 0.521664 [ 6400/60000]\n",
            "loss: 0.579146 [12800/60000]\n",
            "loss: 0.806469 [19200/60000]\n",
            "loss: 0.505725 [25600/60000]\n",
            "loss: 0.596084 [32000/60000]\n",
            "loss: 0.373768 [38400/60000]\n",
            "loss: 0.489028 [44800/60000]\n",
            "loss: 0.535641 [51200/60000]\n",
            "loss: 0.474122 [57600/60000]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.334182 [    0/60000]\n",
            "loss: 0.387934 [ 6400/60000]\n",
            "loss: 0.267466 [12800/60000]\n",
            "loss: 0.524363 [19200/60000]\n",
            "loss: 0.425114 [25600/60000]\n",
            "loss: 0.422319 [32000/60000]\n",
            "loss: 0.347006 [38400/60000]\n",
            "loss: 0.348663 [44800/60000]\n",
            "loss: 0.325704 [51200/60000]\n",
            "loss: 0.315863 [57600/60000]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.361966 [    0/60000]\n",
            "loss: 0.460507 [ 6400/60000]\n",
            "loss: 0.501422 [12800/60000]\n",
            "loss: 0.208240 [19200/60000]\n",
            "loss: 0.295114 [25600/60000]\n",
            "loss: 0.636928 [32000/60000]\n",
            "loss: 0.109247 [38400/60000]\n",
            "loss: 0.345030 [44800/60000]\n",
            "loss: 0.341188 [51200/60000]\n",
            "loss: 0.572901 [57600/60000]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.400532 [    0/60000]\n",
            "loss: 0.310308 [ 6400/60000]\n",
            "loss: 0.307975 [12800/60000]\n",
            "loss: 0.307900 [19200/60000]\n",
            "loss: 0.185063 [25600/60000]\n",
            "loss: 0.212526 [32000/60000]\n",
            "loss: 0.165577 [38400/60000]\n",
            "loss: 0.352011 [44800/60000]\n",
            "loss: 0.317292 [51200/60000]\n",
            "loss: 0.266835 [57600/60000]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.235972 [    0/60000]\n",
            "loss: 0.400160 [ 6400/60000]\n",
            "loss: 0.313922 [12800/60000]\n",
            "loss: 0.329773 [19200/60000]\n",
            "loss: 0.304863 [25600/60000]\n",
            "loss: 0.235292 [32000/60000]\n",
            "loss: 0.217806 [38400/60000]\n",
            "loss: 0.265073 [44800/60000]\n",
            "loss: 0.292801 [51200/60000]\n",
            "loss: 0.381928 [57600/60000]\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qP2-JxxcWVLc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}